{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader ECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le ECL dataset est un dataset sur la consommation d'electricité de 321 clients toutes les heures entre 2012 et 2014.   \n",
    "Les colonnes représentent les 321 clients et il y a 26 304 lignes. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecl_data = pd.read_csv('electricity.csv', header=None).to_numpy()\n",
    "\n",
    "# Diviser les données en ensembles de train, test et validation\n",
    "train_data = ecl_data[:18317]\n",
    "test_data = ecl_data[18317:18317+2633]\n",
    "val_data = ecl_data[18317+2633:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonDataLoader(Dataset):\n",
    "  def __init__(self, data, lookback_size, lookforward_size):\n",
    "    self.lookback_size = lookback_size\n",
    "    self.lookforward_size = lookforward_size\n",
    "    self.data = []\n",
    "    for i in range(0, 1 + len(data) - (self.lookback_size+ self.lookforward_size), self.lookback_size):\n",
    "      seq_x = data[i:i+self.lookback_size, :]\n",
    "      seq_y = data[i+self.lookback_size:i+self.lookback_size+self.lookforward_size, :]\n",
    "      self.data.append((seq_x, seq_y))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    seq_x, seq_y = self.data[idx]\n",
    "    return torch.tensor(seq_x), torch.tensor(seq_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles de train, test et validation\n",
    "train_data = ecl_data[:18317]\n",
    "test_data = ecl_data[18317:18317+2633]\n",
    "val_data = ecl_data[18317+2633:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = MonDataLoader(ecl_data[:18317], 96, 96)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "eval_dataset = MonDataLoader(ecl_data[18317+2633:], 96, 96)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = MonDataLoader(ecl_data[18317+2633:], 96, 96)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96, 321])\n",
      "torch.Size([32, 96, 321])\n",
      "torch.Size([22, 96, 321])\n",
      "torch.Size([22, 96, 321])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/bmqxr1zd3lxd_kcxxw9physr0000gn/T/ipykernel_54672/1951665889.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(seq_x), torch.tensor(seq_y)\n"
     ]
    }
   ],
   "source": [
    "for x,y in test_loader: \n",
    "    print(x.shape)\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAttention(nn.Module):\n",
    "  def __init__(self, k, heads, mask=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param k: taille de l'embeding\n",
    "    :param heads: nombre de heads\n",
    "    :param mask: attention sur toute la seq(False) ou les elts precs seulement\n",
    "    \"\"\"\n",
    "    \n",
    "    super(SAttention, self).__init__()\n",
    "    \n",
    "    assert k % heads == 0, f'heads ({heads}) doit etre un diviseur de k ({k})'\n",
    "\n",
    "    self.k = k\n",
    "    self.heads = heads\n",
    "    self.mask = mask\n",
    "    \n",
    "    self.tokeys = nn.Linear(k, k, bias=False)\n",
    "    self.toqueries = nn.Linear(k, k, bias=False)\n",
    "    self.tovalues = nn.Linear(k, k, bias=False)\n",
    "\n",
    "    self.unifyHeads = nn.Linear(k,k)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    \n",
    "    b, t, k = x.size()\n",
    "    h = self.heads\n",
    "    assert k == self.k, f'Taille des embeddings ({k}) doit correspond a celui du init ({self.k})'\n",
    "\n",
    "    keys = self.tokeys(x)\n",
    "    queries = self.toqueries(x)\n",
    "    values = self.tovalues(x)\n",
    "    \n",
    "    # Taille de chaque head\n",
    "    s = k // h\n",
    "    \n",
    "    # batch x longueur seq x nb heads x taille head\n",
    "    keys = keys.view(b, t, h, s)\n",
    "    queries = queries.view(b, t, h, s)\n",
    "    values = values.view(b, t, h, s)\n",
    "    \n",
    "    # batch et head cote à cote\n",
    "    keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "    queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "    values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "    dot = torch.bmm(queries, keys.transpose(1, 2)) # -> (b*h, t, t)\n",
    "    dot = dot / (k ** (1/2))\n",
    "    \n",
    "    if self.mask:\n",
    "      indices = torch.triu_indices(t, t, offset=1)\n",
    "      dot[:, indices[0], indices[1]] = float('-inf')\n",
    "\n",
    "    dot = F.softmax(dot, dim=2)\n",
    "    \n",
    "    out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "\n",
    "    # rearrangement\n",
    "    out = out.transpose(1, 2).contiguous().view(b, t, h*s)\n",
    "\n",
    "    # unification avec MLP\n",
    "    out = self.unifyHeads(out) # -> (b, t, k)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def forward_einsum(self, x):\n",
    "    b, t, e = x.size()\n",
    "    h = self.heads\n",
    "\n",
    "    keys    = self.tokeys(x).view(b, t, h, e)\n",
    "    queries = self.toqueries(x).view(b, t, h, e)\n",
    "    values  = self.tovalues(x).view(b, t, h, e)\n",
    "\n",
    "    dot = torch.einsum('bthe,bihe->bhti', queries, keys) / math.sqrt(e)\n",
    "    dot = F.softmax(dot, dim=-1)\n",
    "\n",
    "    out = torch.einsum('bhtd,bdhe->bthe', dot, values)\n",
    "\n",
    "    # we can move reshape of weights to init; I left it here just to compare with the original implementation\n",
    "    out = torch.einsum('bthe,khe->btk', out, self.unifyheads.weight.view(e,h,e)) \n",
    "    return out + self.unifyheads.bias\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBlock(nn.Module):\n",
    "  def __init__(self, k, heads):\n",
    "    super(TBlock, self).__init__()\n",
    "    \n",
    "    self.attention = SAttention(k, heads)\n",
    "    self.norm1 = nn.LayerNorm(k)\n",
    "    self.norm2 = nn.LayerNorm(k)\n",
    "\n",
    "    self.fedForward = nn.Sequential(\n",
    "      nn.Linear(k, 4*k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4*k, k)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    attention = self.attention(x)\n",
    "    x = x + attention\n",
    "    x = self.norm1(x)\n",
    "    \n",
    "    fedForward = self.fedForward(x)\n",
    "    x = x + fedForward\n",
    "    out = self.norm2(x)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iTransformer(nn.Module):\n",
    "  def __init__(self, lookback_size, lookforward_size, nb_mvs, heads=4, nb_TBlocks=6, name=None):\n",
    "    super(iTransformer, self).__init__()\n",
    "    \n",
    "    self.k = lookback_size  # nb sequence(mul var) devient la taille de l'embed\n",
    "    self.t = nb_mvs # nb series multivariees devient la taille de la sequence\n",
    "    self.heads = heads\n",
    "    self.nb_TBlocks = nb_TBlocks\n",
    "    self.name = name\n",
    "    \n",
    "    self.pos_emb = nn.Sequential(\n",
    "      nn.Linear(lookback_size, 4 * lookback_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4 * lookback_size, self.k)\n",
    "    )\n",
    "\n",
    "    tblocks = [TBlock(k=self.k, heads=heads) for _ in range(nb_TBlocks)]\n",
    "    self.tblocks = nn.Sequential(*tblocks)\n",
    "    \n",
    "    # projection\n",
    "    self.projection = nn.Sequential(\n",
    "      nn.Linear(self.k, 4 * self.k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4 * self.k, 8 * self.k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(8 * self.k, 16 * self.k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(16 * self.k, lookforward_size * self.t)\n",
    "    ) # -> (lookforward_size * nb_mvs)\n",
    "    \n",
    "    print('Constructeur de iTransformer')\n",
    "    print(f'lb {lookback_size} lf {lookforward_size} mvs {nb_mvs}')\n",
    "  \n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      x (b, lookbacksize, nb_mvs): entree batch de sequences multi variees\n",
    "    \"\"\"\n",
    "    print('Forward iTransformer')\n",
    "    print(f'size batch avant transpose {x.size()}')\n",
    "    x = x.transpose(1, 2).contiguous()  # inverted transformer\n",
    "    b, t, k = x.size()\n",
    "    print(f'size batch avant transpose {x.size()}')\n",
    "    assert t == self.t, f'Taille de la sequence ({t}) doit correspondre a la dim multivariee({self.t})'\n",
    "    assert k == self.k, f'Taille lookback ({k}) doit correspondre à k({self.k}) qui devient la taille de embs'\n",
    "    \n",
    "\n",
    "    pos = torch.arange(k)\n",
    "    print(f'size pos {pos.size()}')\n",
    "    pos = self.pos_emb(pos)#[None, :, :]\n",
    "    print(f'size pos {pos.size()}')\n",
    "    # .expand(b, t, k)\n",
    "\n",
    "    x = x + pos\n",
    "    x = self.tblocks(x)\n",
    "\n",
    "    # prediction  (b, out_seq_len, k)\n",
    "    out = self.projection(x)\n",
    "    out = out.view(self.lookforward_size, self.t)\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96, 321])\n",
      "torch.Size([32, 48, 321])\n"
     ]
    }
   ],
   "source": [
    "ecl_data = np.random.randn(26304, 321)\n",
    "\n",
    "train_dataset = MonDataLoader(ecl_data[:18317], 96, 48)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for seq_x, seq_y in train_loader:\n",
    "  print(seq_x.size())\n",
    "  print(seq_y.size())\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructeur de iTransformer\n",
      "lb 96 lf 48 mvs 321\n"
     ]
    }
   ],
   "source": [
    "nb_clients = 321\n",
    "heads = 4 # 24 * 4 = 96 quand on transpose la sequence son nombre de lignes devient son nombre de colonnes\n",
    "lookback_size = 96\n",
    "lookforward_size = 48\n",
    "lr = 3e-4\n",
    "num_epochs = 3\n",
    "\n",
    "ecl_iTransformer = iTransformer(lookback_size=lookback_size, lookforward_size=lookforward_size, nb_mvs=nb_clients, heads=4, nb_TBlocks=6, name='ECL')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ecl_iTransformer.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward iTransformer\n",
      "size batch avant transpose torch.Size([22, 96, 321])\n",
      "size batch avant transpose torch.Size([22, 321, 96])\n",
      "size pos torch.Size([96])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_x, seq_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 2\u001b[0m   seq_out \u001b[38;5;241m=\u001b[39m \u001b[43mecl_iTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[57], line 51\u001b[0m, in \u001b[0;36miTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(k)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize pos \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#[None, :, :]\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize pos \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# .expand(b, t, k)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "\n",
    "for seq_x, seq_y in train_loader:\n",
    "  seq_out = ecl_iTransformer(x)\n",
    "  \n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
