{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader ECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le ECL dataset est un dataset sur la consommation d'electricité de 321 clients toutes les heures entre 2012 et 2014.   \n",
    "Les colonnes représentent les 321 clients et il y a 26 304 lignes. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecl_data = pd.read_csv('electricity.csv', header=None).to_numpy()\n",
    "\n",
    "# Diviser les données en ensembles de train, test et validation\n",
    "train_data = ecl_data[:18317]\n",
    "test_data = ecl_data[18317:18317+2633]\n",
    "val_data = ecl_data[18317+2633:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonDataLoader(Dataset):\n",
    "  def __init__(self, data, lookback_size, lookforward_size):\n",
    "    self.lookback_size = lookback_size\n",
    "    self.lookforward_size = lookforward_size\n",
    "    self.data = []\n",
    "    for i in range(0, 1 + len(data) - (self.lookback_size+ self.lookforward_size), self.lookback_size):\n",
    "      seq_x = data[i:i+self.lookback_size, :]\n",
    "      seq_y = data[i+self.lookback_size:i+self.lookback_size+self.lookforward_size, :]\n",
    "      self.data.append((seq_x, seq_y))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    seq_x, seq_y = self.data[idx]\n",
    "    return torch.tensor(seq_x), torch.tensor(seq_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles de train, test et validation\n",
    "train_data = ecl_data[:18317]\n",
    "test_data = ecl_data[18317:18317+2633]\n",
    "val_data = ecl_data[18317+2633:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = MonDataLoader(ecl_data[:18317], 96, 96)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "eval_dataset = MonDataLoader(ecl_data[18317+2633:], 96, 96)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = MonDataLoader(ecl_data[18317+2633:], 96, 96)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96, 321])\n",
      "torch.Size([32, 96, 321])\n",
      "torch.Size([22, 96, 321])\n",
      "torch.Size([22, 96, 321])\n"
     ]
    }
   ],
   "source": [
    "for x,y in test_loader: \n",
    "    print(x.shape)\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAttention(nn.Module):\n",
    "  def __init__(self, k, heads, mask=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param k: taille de l'embeding\n",
    "    :param heads: nombre de heads\n",
    "    :param mask: attention sur toute la seq(False) ou les elts precs seulement\n",
    "    \"\"\"\n",
    "    \n",
    "    super(SAttention, self).__init__()\n",
    "\n",
    "    self.k = k\n",
    "    self.heads = heads\n",
    "    self.mask = mask\n",
    "    \n",
    "    self.tokeys = nn.Linear(k, k, bias=False)\n",
    "    self.toqueries = nn.Linear(k, k, bias=False)\n",
    "    self.tovalues = nn.Linear(k, k, bias=False)\n",
    "\n",
    "    self.unifyHeads = nn.Linear(k,k)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    \n",
    "    b, t, k = x.size()\n",
    "    h = self.heads\n",
    "    assert k == self.k, f'Taille des embeddings ({k}) doit correspond a celui du init ({self.k})'\n",
    "\n",
    "    keys = self.tokeys(x)\n",
    "    queries = self.toqueries(x)\n",
    "    values = self.tovalues(x)\n",
    "    \n",
    "    # Taille de chaque head\n",
    "    s = k // h\n",
    "    \n",
    "    # batch x longueur seq x nb heads x taille head\n",
    "    keys = keys.view(b, t, h, s)\n",
    "    queries = queries.view(b, t, h, s)\n",
    "    values = values.view(b, t, h, s)\n",
    "    \n",
    "    # batch et head cote à cote\n",
    "    keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "    queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "    values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "    dot = torch.bmm(queries, keys.transpose(1, 2)) # -> (b*h, t, t)\n",
    "    dot = dot / (k ** (1/2))\n",
    "    \n",
    "    if self.mask:\n",
    "      indices = torch.triu_indices(t, t, offset=1)\n",
    "      dot[:, indices[0], indices[1]] = float('-inf')\n",
    "\n",
    "    dot = F.softmax(dot, dim=2)\n",
    "    \n",
    "    out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "\n",
    "    # rearrangement\n",
    "    out = out.transpose(1, 2).contiguous().view(b, t, h*s)\n",
    "\n",
    "    # unification avec MLP\n",
    "    out = self.unifyHeads(out) # -> (b, t, k)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def forward_einsum(self, x):\n",
    "    b, t, e = x.size()\n",
    "    h = self.heads\n",
    "\n",
    "    keys    = self.tokeys(x).view(b, t, h, e)\n",
    "    queries = self.toqueries(x).view(b, t, h, e)\n",
    "    values  = self.tovalues(x).view(b, t, h, e)\n",
    "\n",
    "    dot = torch.einsum('bthe,bihe->bhti', queries, keys) / math.sqrt(e)\n",
    "    dot = F.softmax(dot, dim=-1)\n",
    "\n",
    "    out = torch.einsum('bhtd,bdhe->bthe', dot, values)\n",
    "\n",
    "    # we can move reshape of weights to init; I left it here just to compare with the original implementation\n",
    "    out = torch.einsum('bthe,khe->btk', out, self.unifyheads.weight.view(e,h,e)) \n",
    "    return out + self.unifyheads.bias\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBlock(nn.Module):\n",
    "  def __init__(self, k, heads):\n",
    "    super(TBlock, self).__init__()\n",
    "    \n",
    "    self.attention = SAttention(k, heads)\n",
    "    self.norm1 = nn.LayerNorm(k)\n",
    "    self.norm2 = nn.LayerNorm(k)\n",
    "\n",
    "    self.fedForward = nn.Sequential(\n",
    "      nn.Linear(k, 4*k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4*k, k)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    attention = self.attention(x)\n",
    "    x = x + attention\n",
    "    x = self.norm1(x)\n",
    "    \n",
    "    fedForward = self.fedForward(x)\n",
    "    x = x + fedForward\n",
    "    out = self.norm2(x)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, k, heads, nb_TBlocks, in_seq_len, out_seq_len):\n",
    "    # in_seq_len pas interessant ici\n",
    "    \n",
    "    self.k = k\n",
    "    self.heads = heads\n",
    "    self.nb_TBlocks = nb_TBlocks\n",
    "    self.in_seq_len = in_seq_len\n",
    "    self.out_seq_len = out_seq_len\n",
    "    \n",
    "    self.pos_emb = nn.Embedding(in_seq_len, k)\n",
    "\n",
    "    tblocks = [TBlock(k=k, heads=heads) for _ in range(nb_TBlocks)]\n",
    "    self.tblocks = nn.Sequential(*tblocks)\n",
    "    \n",
    "    # projection\n",
    "    self.projection = nn.Sequential(\n",
    "      nn.Linear(k, 4*k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(k, 8*k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(8*k, 16*k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(16*k, out_seq_len*k)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      x (b, t, k): entree batch de seq(taille in_seq_len), chaque elt de seq de taille (k)\n",
    "    \"\"\"\n",
    "    b, t, k = x.size()\n",
    "    assert t == self.in_seq_len, f'Taille de la sequence ({t}) doit correspondre à in_seq_len({self.in_seq_len})'\n",
    "    assert k == self.k, f'Taille emb ({k}) doit correspondre à k({self.k})'\n",
    "    \n",
    "\n",
    "    pos = torch.arange(t)\n",
    "    pos = self.pos_emb(pos)[None, :, :].expand(b, t, k)\n",
    "\n",
    "    x = x + pos\n",
    "    x = self.tblocks(x)\n",
    "\n",
    "    # prediction  (b, out_seq_len, k)\n",
    "    out = self.projection(x)\n",
    "    out = out.view(self.out_seq_len, self.k)\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
